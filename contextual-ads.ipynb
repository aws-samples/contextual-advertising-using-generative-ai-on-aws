{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6416fcdf-75c5-49fc-a9ec-adb8018df284",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Workshop: Scene level contextual understanding of media content using Generative AI\n",
    "\n",
    "## Notebook configuration\n",
    "- Image: Data Science 3.0\n",
    "- Instance Type: ml.m5.2xlarge (Recommended)\n",
    "- Python version: 3.10\n",
    "\n",
    "## Rundown of the workshop\n",
    "1. Prerequisite\n",
    "    - Import helper modules\n",
    "    - Installing python packages (opencv, faiss-cpu, webvtt-py, termcolor\n",
    "    - Downloading sample video (Netflix Open Content - Meridian)\n",
    "2. Generating \"chapter points\" based on topic changes in conversation\n",
    "    - Uploading the sample video to Amazon S3 bucket\n",
    "    - Converting speech to text with Amazon Transcribe\n",
    "    - Using Anthropic Claude 3 Haiku to analyse the conversation\n",
    "    - Validating \"chapter points\" timestamps\n",
    "3. Creating visual scenes from sample video\n",
    "    - Frame extraction\n",
    "    - Grouping frames into shots with Amazon Titan Multimodal Embedding\n",
    "    - Grouping shots into scenes with vector store\n",
    "4. Generating chapter level contextual information\n",
    "    - Grouping scenes into chapters with both visual and audio analysis\n",
    "    - Contextual understandings with Anthropic Claude 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488c3b7-2901-497e-8a1e-10d768f31943",
   "metadata": {},
   "source": [
    "## Install python packages\n",
    "\n",
    "- opencv for video and image processing\n",
    "- faiss for vector store\n",
    "- webvtt-py for parsing subtitle file\n",
    "- termcolor for formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc08d0-dbc2-423e-8476-bdad70f87393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python termcolor faiss-cpu webvtt-py \n",
    "# !pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453ab94-9b8a-42fe-a0e5-2b2e742d71fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from termcolor import colored\n",
    "import inspect\n",
    "import time\n",
    "import cv2\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from termcolor import colored\n",
    "import glob\n",
    "import os\n",
    "from functools import cmp_to_key\n",
    "\n",
    "\n",
    "from lib import transcribe_helper as trh\n",
    "from lib import s3_helper as s3h\n",
    "from lib import chapters as chpt\n",
    "from lib import util\n",
    "from lib import embeddings\n",
    "from lib import frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db107e-04e8-4604-9286-dac677a2eed8",
   "metadata": {},
   "source": [
    "## Download the sample video, Meridian, from Netflix\n",
    "\n",
    "The open source content is available under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aca4dd-42c6-4da0-94c5-263e7bd42ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "mp4_file = 'Netflix_Open_Content_Meridian.mp4'\n",
    "url = f\"https://dx2y1cac29mt3.cloudfront.net/mp4/netflix/{mp4_file}\"\n",
    "\n",
    "!curl {url} -o {mp4_file}\n",
    "\n",
    "Video(mp4_file, width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf4d06-855a-41e3-8fc6-716db94c024b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate chapter segments based on the dialog in the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aac370-2f9f-4989-b437-b9ca58825b22",
   "metadata": {},
   "source": [
    "### Get Sagemaker default resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7344ae2-2380-4789-94ee-0094007e7f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sagemaker_resources = {}\n",
    "sagemaker_resources[\"session\"] = sagemaker.Session()\n",
    "sagemaker_resources[\"bucket\"] = sagemaker_resources[\"session\"].default_bucket()\n",
    "sagemaker_resources[\"role\"] = sagemaker.get_execution_role()\n",
    "sagemaker_resources[\"region\"] = sagemaker_resources[\"session\"]._region_name\n",
    "\n",
    "print(sagemaker_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644e0bb-3d2f-4668-b69e-a1ad18c2eecd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload the sample video to the default Amazon S3 bucket for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd992c-c1be-4f98-a60c-dc0d87df5f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "response = s3h.upload_object(sagemaker_resources[\"bucket\"], \"contextual_ad\", mp4_file) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05b011-a6e9-4d89-a122-1bd4d881c568",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use Amazon Transcribe to convert speech to text\n",
    "\n",
    "This section uses Amazon Transcribe to convert the speech to text and generate a WebVTT output.\n",
    "\n",
    "If you are getting `AccessDeniedException`, log on to `AWS IAM Console`, find the SageMaker Execution IAM Role, and add the following managed polices:\n",
    "- AmazonTranscribeFullAccess\n",
    "- AmazonRekognitionFullAccess\n",
    "- AmazonBedrockFullAccess\n",
    "\n",
    "Also check out the pricing on [Amazon Transcribe Pricing](https://aws.amazon.com/transcribe/pricing/) in us-east-1 region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12d43a-ea2c-4e20-bda1-8265ae19b4c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'SageMaker execution IAM Role ARN: {sagemaker_resources[\"role\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e15b6-ee3a-4884-ae87-0dd6aafdaa24",
   "metadata": {},
   "source": [
    "### Start the transcription job and wait for it to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad04daa-1a25-46ca-8822-fe9792fd111b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start transcription job\n",
    "\n",
    "def transcribe(bucket, path, file, media_format=\"mp4\", language_code=\"en-US\", verbose=True):\n",
    "\n",
    "    # start transcription job\n",
    "    transcribe_response = start_transcription_job(\n",
    "        bucket, \n",
    "        path,\n",
    "        file, media_format, language_code)\n",
    "\n",
    "    # wait for completion\n",
    "    transcribe_response = trh.wait_for_transcription_job(\n",
    "        transcribe_response['TranscriptionJob']['TranscriptionJobName'], \n",
    "        verbose)\n",
    "\n",
    "    return transcribe_response\n",
    "\n",
    "transcribe_response = trh.transcribe(sagemaker_resources[\"bucket\"], \"contextual_ad\", mp4_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb594a-a188-425e-9f45-722fd0243b3a",
   "metadata": {},
   "source": [
    "### Examine the results from Amazon Transcribe\n",
    "\n",
    "The response from Amazon Transcribe contains a `results` dictionary with a `transcript` that contains a text-only transcript and a collection of `items` which contain each word and punctuation in the transcript along with a confidence score and timestamp for the item. The response also contains the same transcript formatted as subtitles in either WebVTT or SRT format.  Let's take a look at these outputs.  \n",
    "\n",
    "We will be using the WebVTT output for our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f39a8-2b3b-4782-a617-dad8f3d0752e",
   "metadata": {},
   "source": [
    "**Transcript**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75fdf1-996f-499a-9f3a-6aee39c71062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_filename = trh.download_transcript(transcribe_response)\n",
    "\n",
    "JSON(filename=transcript_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1582f-2dde-44d6-aed4-b9ce880e0557",
   "metadata": {
    "tags": []
   },
   "source": [
    "**WebVTT Subtitles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03f27e-2830-4db8-86dc-44a267878f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vtt_filename = trh.download_vtt(transcribe_response)\n",
    "\n",
    "!head transcript.vtt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6277d-19bf-41b2-804e-0144f3597914",
   "metadata": {},
   "source": [
    "### Estimate the cost of the transcription job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107506fb-2992-4102-aa1e-2bbd8b57ef8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "transcribe_cost = trh.display_transcription_cost(mp4_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc410f93-9bb7-416c-bffc-59e4dd33fb5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(inspect.getsource(trh.display_transcription_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaef380-0974-48c3-866a-82c47f366d28",
   "metadata": {},
   "source": [
    "### Use an Amazon Bedrock to generate chapters from the subtitles \n",
    "This section demonstrates using LLM to breakdown the conversations based on topic changes. It uses Anthropic Claude 3 Haiku model via Amazon Bedrock service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b1ff6-0b91-489c-8b9a-0438292f7b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run LLM model\n",
    "import json\n",
    "from urllib.request import urlretrieve\n",
    "import boto3\n",
    "from termcolor import colored\n",
    "\n",
    "def make_conversation_example():\n",
    "    example = {\n",
    "        'chapters': [\n",
    "            {\n",
    "                'start': '00:00:10.000',\n",
    "                'end': '00:00:32.000',\n",
    "                'reason': 'It appears the chapter talks about...'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': 'JSON format. An example of the output:\\n{0}\\n'.format(json.dumps(example))\n",
    "    }\n",
    "\n",
    "def make_transcript(transcript_file):\n",
    "    with open(transcript_file) as f:\n",
    "        transcript = f.read()\n",
    "    \n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': 'Here is the transcripts in <transcript> tag:\\n<transcript>{0}\\n</transcript>\\n'.format(transcript)\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_conversations(transcript_file):\n",
    "    messages = []\n",
    "\n",
    "    # transcript\n",
    "    transcript_message = make_transcript(transcript_file)\n",
    "    messages.append(transcript_message)\n",
    "\n",
    "    # output format?\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'Got the transcript. What output format?'\n",
    "    })\n",
    "\n",
    "    # example output\n",
    "    example_message = make_conversation_example()\n",
    "    messages.append(example_message)\n",
    "\n",
    "    # prefill output\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': '{'\n",
    "    })\n",
    "\n",
    "    # model parameters\n",
    "    model_id = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "    model_ver = 'bedrock-2023-05-31'\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "    ## system prompt to role play\n",
    "    system = 'You are a media operation assistant who analyses movie transcripts in WebVTT format and suggest chapter points based on the topic changes in the conversations. It is important to read the entire transcripts.'\n",
    "\n",
    "    ## setting up the model params\n",
    "    model_params = {\n",
    "        'anthropic_version': model_ver,\n",
    "        'max_tokens': 4096,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.7,\n",
    "        'top_k': 20,\n",
    "        'stop_sequences': ['\\n\\nHuman:'],\n",
    "        'system': system,\n",
    "        'messages': messages\n",
    "    }\n",
    "\n",
    "    bedrock_runtime_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps(model_params),\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    # patch the json string output with '{' and parse it\n",
    "    response_content = response_body['content'][0]['text']\n",
    "    if response_content[0] != '{':\n",
    "        response_content = '{' + response_content\n",
    "    response_content = json.loads(response_content)\n",
    "    response_body['content'][0]['json'] = response_content\n",
    "\n",
    "    return response_body\n",
    "\n",
    "def display_conversation_cost(response):\n",
    "    # us-east-1 pricing\n",
    "    input_per_1k = 0.00025\n",
    "    output_per_1k = 0.00125\n",
    "\n",
    "    input_tokens = response['usage']['input_tokens']\n",
    "    output_tokens = response['usage']['output_tokens']\n",
    "\n",
    "    conversation_cost = (\n",
    "        input_per_1k * input_tokens +\n",
    "        output_per_1k * output_tokens\n",
    "    ) / 1000\n",
    "\n",
    "    print('\\n')\n",
    "    print('========================================================================')\n",
    "    print('Estimated cost:', colored(f\"${conversation_cost}\", 'green'), f\"in us-east-1 region with {colored(input_tokens, 'green')} input tokens and {colored(output_tokens, 'green')} output tokens.\")\n",
    "    print('========================================================================')\n",
    "\n",
    "    return {\n",
    "        'input_per_1k': input_per_1k,\n",
    "        'output_per_1k': output_per_1k,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'estimated_cost': conversation_cost,\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_conversations(transcript_file):\n",
    "    messages = []\n",
    "\n",
    "    # transcript\n",
    "    transcript_message = make_transcript(transcript_file)\n",
    "    messages.append(transcript_message)\n",
    "\n",
    "    # output format?\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'Got the transcript. What output format?'\n",
    "    })\n",
    "\n",
    "    # example output\n",
    "    example_message = make_conversation_example()\n",
    "    messages.append(example_message)\n",
    "\n",
    "    # prefill output\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': '{'\n",
    "    })\n",
    "\n",
    "    # model parameters\n",
    "    model_id = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "    model_ver = 'bedrock-2023-05-31'\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "    ## system prompt to role play\n",
    "    system = 'You are a media operation assistant who analyses movie transcripts in WebVTT format and suggest chapter points based on the topic changes in the conversations. It is important to read the entire transcripts.'\n",
    "\n",
    "    ## setting up the model params\n",
    "    model_params = {\n",
    "        'anthropic_version': model_ver,\n",
    "        'max_tokens': 4096,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.7,\n",
    "        'top_k': 20,\n",
    "        'stop_sequences': ['\\n\\nHuman:'],\n",
    "        'system': system,\n",
    "        'messages': messages\n",
    "    }\n",
    "\n",
    "    bedrock_runtime_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps(model_params),\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    # patch the json string output with '{' and parse it\n",
    "    response_content = response_body['content'][0]['text']\n",
    "    if response_content[0] != '{':\n",
    "        response_content = '{' + response_content\n",
    "    response_content = json.loads(response_content)\n",
    "    response_body['content'][0]['json'] = response_content\n",
    "\n",
    "    return response_body\n",
    "\n",
    "conversation_response = analyze_conversations(vtt_filename)\n",
    "\n",
    "# show the conversation cost\n",
    "conversation_cost = display_conversation_cost(conversation_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990df82c-eb67-4a84-8ce7-4499cb44f326",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's take a look at the conversations that were generated from the transcript "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906eced6-682a-4578-8a60-a179b93d529b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversations = conversation_response['content'][0]['json']\n",
    "\n",
    "JSON(conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c93f658-0d0f-4513-ba36-44bc843d1c0b",
   "metadata": {},
   "source": [
    "### Generating \"chapter points\" #4: Validating \"chapter points\" timestamps\n",
    "\n",
    "While LLM can help to break down the conversations, we need to ensure that the timestamps generated are indeed valid. This can be done by matching the timestamp boundaries against the timestamps of the \"dialogues\" from the original WebVTT file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e81a9-b656-4d35-bc97-6b9bcc5ba076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Validating the timestamp boundaries of the conversations against the WebVtt timestamps\n",
    "def validate_timestamps(chapters, captions):\n",
    "    ## collect caption timestamps per chapter\n",
    "    for chapter in chapters:\n",
    "        chapter_start = chapter['start_ms']\n",
    "        chapter_end = chapter['end_ms']\n",
    "\n",
    "        while len(captions) > 0:\n",
    "            caption = captions[0]\n",
    "\n",
    "            caption_start = caption['start_ms']\n",
    "            caption_end = caption['end_ms']\n",
    "\n",
    "            if caption_start >= chapter_end:\n",
    "                break\n",
    "\n",
    "            if caption_end <= chapter_start:\n",
    "                captions.pop(0)\n",
    "                continue\n",
    "\n",
    "            if abs(chapter_end - caption_start) < abs(caption_end - chapter_end):\n",
    "                break\n",
    "\n",
    "            if 'timestamps' not in chapter:\n",
    "                chapter['timestamps'] = []\n",
    "            chapter['timestamps'].append([caption_start, caption_end])\n",
    "\n",
    "            captions.pop(0)\n",
    "\n",
    "    ## align the chapter boundary timestamps with the caption timestamps\n",
    "    for chapter in chapters:\n",
    "        if 'timestamps' not in chapter:\n",
    "            continue\n",
    "        \n",
    "        chapter_start = chapter['start_ms']\n",
    "        chapter_end = chapter['end_ms']\n",
    "\n",
    "        caption_start = chapter['timestamps'][0][0]\n",
    "        caption_end = chapter['timestamps'][-1][1]\n",
    "\n",
    "        if chapter_start != caption_start:\n",
    "            chapter['start_ms'] = caption_start\n",
    "            chapter['start'] = to_hhmmssms(caption_start)\n",
    "\n",
    "        if chapter_end != caption_end:\n",
    "            chapter['end_ms'] = caption_end\n",
    "            chapter['end'] = to_hhmmssms(caption_end)\n",
    "\n",
    "        del chapter['timestamps']\n",
    "\n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdebcd7-3e40-42a9-88ff-9781abba916d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## parse the conversation\n",
    "conversations = conversation_response['content'][0]['json']\n",
    "\n",
    "## merge overlapped conversation timestamps\n",
    "chapters = chpt.merge_chapters(conversations['chapters'])\n",
    "\n",
    "## validate the conversation timestamps against the caption timestamps\n",
    "captions = chpt.parse_webvtt(vtt_filename)\n",
    "chapters = chpt.validate_timestamps(chapters, captions)\n",
    "\n",
    "conversations['chapters'] = chapters\n",
    "\n",
    "print(f\"Number of chapters: {len(conversations['chapters'])}\")\n",
    "print(conversations['chapters'][0])\n",
    "\n",
    "\n",
    "## save the conversations\n",
    "util.save_json_to_file('conversations.json', conversations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3536067-54b4-457c-9402-4eda7c34933f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimated_cost = conversation_cost['estimated_cost'] + transcribe_cost['estimated_cost']\n",
    "estimated_cost = round(estimated_cost, 4)\n",
    "\n",
    "print('\\n')\n",
    "print('Generating \"chapter points\"')\n",
    "print('========================================================================')\n",
    "print('Transcribe cost:', colored(f\"${round(transcribe_cost['estimated_cost'], 4)}\", 'green'), f\"with duration of {colored(transcribe_cost['duration'], 'green')}s\")\n",
    "print('Bedrock cost:', colored(f\"${round(conversation_cost['estimated_cost'], 4)}\", 'green'), f\"with {colored(conversation_cost['input_tokens'], 'green')} input tokens and {colored(conversation_cost['output_tokens'], 'green')} output tokens.\")\n",
    "print('-----')\n",
    "print('Estimated cost:', colored(f\"${estimated_cost}\", 'green'))\n",
    "print('========================================================================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f6196f-74df-4941-950e-e7f030396ec7",
   "metadata": {},
   "source": [
    "## CHECKPOINT\n",
    "\n",
    "At this point, we have taken the audio part of the video file, run Amazon Transcribe to convert the speech to text, and run Amazon Bedrock (Anthropic Claude 3 Haiku) model to analyze the conversations.\n",
    "\n",
    "Let's move on to analyzing the visual part of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7871c9-11aa-4ddc-bd9b-2a2ec0931e7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating visual scenes from sample video #1: Frame extraction\n",
    "\n",
    "In this section, we are extracting 1 frame per second with a resolution of `392x220` from the sample video. Using `392x220` is chosen for a reason and will be discussed in \"Generating chapter level contextual information\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dabef4-a714-43c9-96b4-8a5359ec6f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_frames(video_file, size = (392, 220)):\n",
    "    # start fresh\n",
    "    frames.rmdir('frames')\n",
    "\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    print(f\"  total_frames = {total_frames}, fps = {fps}\")\n",
    "\n",
    "    # re-create frames folder\n",
    "    frames.mkdir('frames')\n",
    "\n",
    "    skip_step = round(fps)\n",
    "    processed_frames = 0\n",
    "    saved_frames = 0\n",
    "\n",
    "    while (True):\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        if processed_frames % skip_step == 0:\n",
    "            # print(f\"{saved_frames} / {processed_frames} / {total_frames}\")\n",
    "            name = f\"frames/frame.{saved_frames:07d}.jpg\"\n",
    "            scaled = cv2.resize(frame, size)\n",
    "            cv2.imwrite(name, scaled)\n",
    "            saved_frames += 1\n",
    "\n",
    "        processed_frames += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # return jpeg files\n",
    "    jpeg_frames = sorted(glob.glob('frames/*.jpg'))\n",
    "    return jpeg_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c3d27-06d7-420d-a93c-bb05cea95223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "jpeg_files = extract_frames(mp4_file)\n",
    "\n",
    "print(f\"Frame extracted: {len(jpeg_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b942415-e5b5-48d5-bac2-5d5ce37c3e91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating visual scenes from sample video #2: Grouping frames into shots with Amazon Titan Multimodal Embedding\n",
    "\n",
    "- Generating frame embeddings with Amazon Titan Multimodal Embedding model\n",
    "- Grouping frames into shots with cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be9bc8-5a9d-4848-9143-aa5f8eb0396f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generating frame embeddings with Amazon Titan Multimodal Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684f236-53ba-4540-9377-c3594ec9c602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def batch_generate_embeddings(jpeg_files):\n",
    "    frame_embeddings = []\n",
    "\n",
    "    titan_model_id = 'amazon.titan-embed-image-v1'\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "    bedrock_runtime_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    for jpeg_file in jpeg_files:\n",
    "        #print(f\"== PROCESSING: {jpeg_file}\")\n",
    "\n",
    "        image = Image.open(jpeg_file)\n",
    "        input_image = frames.image_to_base64(image)\n",
    "\n",
    "        model_params = {\n",
    "            'inputImage': input_image,\n",
    "            'embeddingConfig': {\n",
    "                'outputEmbeddingLength': 1024 #384 #256\n",
    "            }\n",
    "        }\n",
    "\n",
    "        body = json.dumps(model_params)\n",
    "\n",
    "        response = bedrock_runtime_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=titan_model_id,\n",
    "            accept=accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "\n",
    "        basename = os.path.basename(jpeg_file)\n",
    "        frame_no = int(basename.split('.')[1])\n",
    "        frame_embeddings.append({\n",
    "            'file': jpeg_file,\n",
    "            'frame_no': frame_no,\n",
    "            'embedding': response_body['embedding']\n",
    "        })\n",
    "\n",
    "    return frame_embeddings\n",
    "\n",
    "def display_embedding_cost(frame_embeddings):\n",
    "    per_image_embedding = 0.00006\n",
    "    estimated_cost = per_image_embedding * len(frame_embeddings)\n",
    "\n",
    "    print('\\n')\n",
    "    print('========================================================================')\n",
    "    print('Estimated cost:', colored(f\"${round(estimated_cost, 4)}\", 'green'), f\"in us-east-1 region with {len(frame_embeddings)} embeddings\")\n",
    "    print('========================================================================')\n",
    "\n",
    "    return {\n",
    "        'per_image_embedding': per_image_embedding,\n",
    "        'estimated_cost': estimated_cost,\n",
    "        'num_embeddings': len(frame_embeddings)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b8675d-83f9-4597-b8b0-62275e32d269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_embeddings = batch_generate_embeddings(jpeg_files)\n",
    "\n",
    "## save the frame embeddings\n",
    "util.save_json_to_file('frame_embeddings.json', frame_embeddings)\n",
    "\n",
    "frame_embeddings_cost = display_embedding_cost(frame_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c729fc7-a7a8-472b-a4b2-c586dddf29a5",
   "metadata": {},
   "source": [
    "#### Grouping adjacent frames into shots with cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac26d20-0dd9-4b07-b437-de29424c95e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def group_frames_to_shots(frame_embeddings, min_similarity = 0.80):\n",
    "    shots = []\n",
    "    current_shot = [frame_embeddings[0]]\n",
    "\n",
    "    # group frames based on the similarity\n",
    "    for i in range(1, len(frame_embeddings)):\n",
    "        prev = current_shot[-1]\n",
    "        cur = frame_embeddings[i]\n",
    "        prev_embedding = prev['embedding']\n",
    "        cur_embedding = cur['embedding']\n",
    "\n",
    "        similarity = embeddings.cosine_similarity(prev_embedding, cur_embedding)\n",
    "        cur['similarity'] = similarity\n",
    "\n",
    "        if similarity > min_similarity:\n",
    "            current_shot.append(cur)\n",
    "        else:\n",
    "            shots.append(current_shot)\n",
    "            current_shot = [cur]\n",
    "\n",
    "    if len(current_shot) > 0:\n",
    "        shots.append(current_shot)\n",
    "\n",
    "    frames_in_shots = []\n",
    "    for i in range(len(shots)):\n",
    "        shot = shots[i]\n",
    "        frames_ids = [frame['frame_no'] for frame in shot]\n",
    "        frames_in_shots.append({\n",
    "            'shot_id': i,\n",
    "            'frame_ids': frames_ids\n",
    "        })\n",
    "\n",
    "    return frames_in_shots\n",
    "\n",
    "\n",
    "def plot_shots(frame_embeddings, num_shots):\n",
    "    try:\n",
    "        os.mkdir('shots')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    shots = [[] for _ in range(num_shots)]\n",
    "    for frame in frame_embeddings:\n",
    "        shot_id = frame['shot_id']\n",
    "        file = frame['file']\n",
    "        shots[shot_id].append(file)\n",
    "\n",
    "    for i in range(len(shots)):\n",
    "        shot = shots[i]\n",
    "        num_frames = len(shot)\n",
    "        skipped_frames = frames.skip_frames(shot)\n",
    "        grid_image = frames.create_grid_image(skipped_frames)\n",
    "        w, h = grid_image.size\n",
    "        if h > 440:\n",
    "            grid_image = grid_image.resize((w // 2, h // 2))\n",
    "        w, h = grid_image.size\n",
    "        print(f\"Shot #{i:04d}: {num_frames} frames ({len(skipped_frames)} drawn) [{w}x{h}]\")\n",
    "        grid_image.save(f\"shots/shot-{i:04d}.jpg\")\n",
    "        display(grid_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe134c1-a469-4d52-be67-839c45d1e45f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames_in_shots = group_frames_to_shots(frame_embeddings)\n",
    "\n",
    "print(f\"Number of shots: {len(frames_in_shots)} from {len(frame_embeddings)} frames\")\n",
    "\n",
    "# update shot_id in frame_embeddings dict\n",
    "for i in range(len(frames_in_shots)):\n",
    "    frames_in_shot = frames_in_shots[i]\n",
    "    for frame_id in frames_in_shot['frame_ids']:\n",
    "        frame_embeddings[frame_id]['shot_id'] = i\n",
    "\n",
    "# save to json file\n",
    "util.save_json_to_file('frames_in_shots.json', frames_in_shots)\n",
    "\n",
    "# overwrite the embeddings with the shot_id\n",
    "util.save_json_to_file('frame_embeddings.json', frame_embeddings)\n",
    "\n",
    "# plot the shot images\n",
    "plot_shots(frame_embeddings, len(frames_in_shots))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33680b6a-3944-4a0c-9bf4-667807afa609",
   "metadata": {},
   "source": [
    "### Creating visual scenes from sample video #3: Grouping shots into scenes with vector store\n",
    "\n",
    "- Creating a local vector store with Faiss and indexing all frame embeddings\n",
    "- Searching simliar frames and groping them into scenes\n",
    "    - The previous step (grouping frames to shots) compares the similarity of the adjacent frames. This step compares the frames to the rest of the frame images of the entire content. This allows us to group frame images that are further apart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827783f-150d-470d-a865-3a1e11e77cdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Creating a local vector store with Faiss and index all frame embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796befa-76a0-4825-bb78-c56e6941d79a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def collect_similar_frames(frame_embeddings, frame_ids):\n",
    "    similar_frames = []\n",
    "    for frame_id in frame_ids:\n",
    "        similar_frames_ids = [frame['idx'] for frame in frame_embeddings[frame_id]['similar_frames']]\n",
    "        similar_frames.extend(similar_frames_ids)\n",
    "    # unique frames in shot\n",
    "    return sorted(list(set(similar_frames)))\n",
    "\n",
    "def collect_related_shots(frame_embeddings, frame_ids):\n",
    "    related_shots = []\n",
    "    for frame_id in frame_ids:\n",
    "        related_shots.append(frame_embeddings[frame_id]['shot_id'])\n",
    "    # unique frames in shot\n",
    "    return sorted(list(set(related_shots)))\n",
    "\n",
    "\n",
    "def group_shots_in_scenes(frames_in_shots):\n",
    "    scenes = [\n",
    "        [\n",
    "            min(frames_in_shot['related_shots']),\n",
    "            max(frames_in_shot['related_shots']),\n",
    "        ] for frames_in_shot in frames_in_shots\n",
    "    ]\n",
    "\n",
    "    scenes = sorted(scenes, key=cmp_to_key(embeddings.cmp_min_max))\n",
    "\n",
    "    stack = [scenes[0]]\n",
    "    for i in range(1, len(scenes)):\n",
    "        prev = stack[-1]\n",
    "        cur = scenes[i]\n",
    "        prev_min, prev_max = prev\n",
    "        cur_min, cur_max = cur\n",
    "\n",
    "        if cur_min >= prev_min and cur_min <= prev_max:\n",
    "            new_scene = [\n",
    "                min(cur_min, prev_min),\n",
    "                max(cur_max, prev_max),\n",
    "            ]\n",
    "            stack.pop()\n",
    "            stack.append(new_scene)\n",
    "            continue\n",
    "            \n",
    "        stack.append(cur)\n",
    "\n",
    "    return [{\n",
    "        'scene_id': i,\n",
    "        'shot_ids': stack[i],\n",
    "    } for i in range(len(stack))]\n",
    "\n",
    "def plot_scenes(frame_embeddings, num_scenes):\n",
    "    try:\n",
    "        os.mkdir('scenes')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    scenes = [[] for _ in range(num_scenes)]\n",
    "    for frame in frame_embeddings:\n",
    "        scene_id = frame['scene_id']\n",
    "        file = frame['file']\n",
    "        scenes[scene_id].append(file)\n",
    "\n",
    "    for i in range(len(scenes)):\n",
    "        scene = scenes[i]\n",
    "        num_frames = len(scene)\n",
    "        skipped_frames = frames.skip_frames(scene)\n",
    "        grid_image = frames.create_grid_image(skipped_frames)\n",
    "        w, h = grid_image.size\n",
    "        if h > 440:\n",
    "            grid_image = grid_image.resize((w // 2, h // 2))\n",
    "        w, h = grid_image.size\n",
    "        print(f\"Scene #{i:04d}: {num_frames} frames ({len(skipped_frames)} drawn) [{w}x{h}]\")\n",
    "        grid_image.save(f\"scenes/scene-{i:04d}.jpg\")\n",
    "        display(grid_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c8403-5416-4504-82d8-3ad16be8b24a",
   "metadata": {},
   "source": [
    "#### Searching simliar frames and grouping them into scenes\n",
    "\n",
    "- Find all similar frames that are within 30 seconds for each of the frames\n",
    "- Collect all unique similar frames from previous step for each shot (a collection of frames) and collect the related shots\n",
    "- Reduction of shots into scenes by checking the overlapping shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac1ade-86e6-4f4f-b3b6-d45d1a89ec01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## create an index\n",
    "dimension = len(frame_embeddings[0]['embedding'])\n",
    "vector_store = embeddings.create_index(dimension)\n",
    "\n",
    "## indexing all the frames\n",
    "embeddings.index_frames(vector_store, frame_embeddings)\n",
    "print(f\"Total indexed = {vector_store.ntotal}\")\n",
    "\n",
    "## find similar frames for each of the frames and store in the frame_embeddings\n",
    "for frame in frame_embeddings:\n",
    "    similar_frames = embeddings.search_similarity(vector_store, frame)\n",
    "    frame['similar_frames'] = similar_frames\n",
    "\n",
    "## find all similar frames that are related to the shots and store in the frames_in_shots\n",
    "for frames_in_shot in frames_in_shots:\n",
    "    similar_frames_in_shot = collect_similar_frames(frame_embeddings, frames_in_shot['frame_ids'])\n",
    "    frames_in_shot['similar_frames_in_shot'] = similar_frames_in_shot\n",
    "\n",
    "    related_shots = collect_related_shots(frame_embeddings, similar_frames_in_shot)\n",
    "    frames_in_shot['related_shots'] = related_shots\n",
    "\n",
    "shots_in_scenes = group_shots_in_scenes(frames_in_shots)\n",
    "\n",
    "# store the scene_id to all structs\n",
    "for scene in shots_in_scenes:\n",
    "    scene_id = scene['scene_id']\n",
    "    shot_min, shot_max = scene['shot_ids']\n",
    "    print(f\"Scene #{scene_id}: {shot_min} - {shot_max} ({shot_max - shot_min + 1})\")\n",
    "    # update json files\n",
    "    for shot_id in range(shot_min, shot_max + 1):\n",
    "        frames_in_shots[shot_id]['scene_id'] = scene_id\n",
    "        for frame_id in frames_in_shots[shot_id]['frame_ids']:\n",
    "            frame_embeddings[frame_id]['scene_id'] = scene_id\n",
    "\n",
    "# update the json files\n",
    "util.save_json_to_file('shots_in_scenes.json', shots_in_scenes)\n",
    "util.save_json_to_file('frames_in_shots.json', frames_in_shots)\n",
    "util.save_json_to_file('frame_embeddings.json', frame_embeddings)\n",
    "\n",
    "# plot the scene images\n",
    "plot_scenes(frame_embeddings, len(shots_in_scenes))\n",
    "\n",
    "print(f\"Number of frames: {len(frame_embeddings)}\")\n",
    "print(f\"Number of shots: {len(frames_in_shots)}\")\n",
    "print(f\"Number of scenes: {len(shots_in_scenes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca044876-3b08-4113-8ab2-448d70c0ca06",
   "metadata": {},
   "source": [
    "### Generating chapter level contextual information #1: Grouping scenes into chapters with both visual and audio analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e020bb-f9c4-46d4-9f65-627986b0dde7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def make_chapter_item(chapter_id, scene_items, text = ''):\n",
    "    scene_ids = [scene['scene_id'] for scene in scene_items]\n",
    "    return {\n",
    "        'chapter_id': chapter_id,\n",
    "        'scene_ids': [min(scene_ids), max(scene_ids)],\n",
    "        'text': text,\n",
    "    }\n",
    "\n",
    "def group_scenes_in_chapters(conversations, shots_in_scenes, frames_in_shots):\n",
    "    scenes = copy.deepcopy(shots_in_scenes)\n",
    "\n",
    "    chapters = []\n",
    "    for conversation in conversations['chapters']:\n",
    "        start_ms = conversation['start_ms']\n",
    "        end_ms = conversation['end_ms']\n",
    "        text = conversation['reason']\n",
    "\n",
    "        stack = []\n",
    "        while len(scenes) > 0:\n",
    "            scene = scenes[0]\n",
    "            shot_min, shot_max = scene['shot_ids']\n",
    "            frame_start = min(frames_in_shots[shot_min]['frame_ids']) * 1000\n",
    "            frame_end = max(frames_in_shots[shot_max]['frame_ids']) * 1000\n",
    "\n",
    "            if frame_start > end_ms:\n",
    "                break\n",
    "\n",
    "            # scenes before any conversation starts\n",
    "            if frame_end < start_ms:\n",
    "                chapter = make_chapter_item(len(chapters), [scene])\n",
    "                chapters.append(chapter)\n",
    "                scenes.pop(0)\n",
    "                continue\n",
    "\n",
    "            stack.append(scene)\n",
    "            scenes.pop(0)\n",
    "\n",
    "        if len(stack) > 0:\n",
    "            chapter = make_chapter_item(len(chapters), stack, text)\n",
    "            chapters.append(chapter)\n",
    "\n",
    "    ## There could be more scenes without converations, append them\n",
    "    for scene in scenes:\n",
    "        chapter = make_chapter_item(len(chapters), [scene])\n",
    "        chapters.append(chapter)\n",
    "\n",
    "    return chapters\n",
    "\n",
    "def plot_chapters(frame_embeddings, num_chapters):\n",
    "    try:\n",
    "        os.mkdir('chapters')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    chapters = [[] for _ in range(num_chapters)]\n",
    "    for frame in frame_embeddings:\n",
    "        chapter_id = frame['chapter_id']\n",
    "        file = frame['file']\n",
    "        chapters[chapter_id].append(file)\n",
    "\n",
    "    for i in range(len(chapters)):\n",
    "        chapter = chapters[i]\n",
    "        num_frames = len(chapter)\n",
    "        skipped_frames = frames.skip_frames(chapter)\n",
    "        grid_image = frames.create_grid_image(skipped_frames)\n",
    "        w, h = grid_image.size\n",
    "        if h > 440:\n",
    "            grid_image = grid_image.resize((w // 2, h // 2))\n",
    "        w, h = grid_image.size\n",
    "        print(f\"Chapter #{i:04d}: {num_frames} frames ({len(skipped_frames)} drawn) [{w}x{h}]\")\n",
    "        grid_image.save(f\"chapters/chapter-{i:04d}.jpg\")\n",
    "        display(grid_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d0967-a52f-4cc4-bfdf-9c2f27db78e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scenes_in_chapters = group_scenes_in_chapters(\n",
    "    conversations,\n",
    "    shots_in_scenes,\n",
    "    frames_in_shots\n",
    ")\n",
    "\n",
    "for scenes_in_chapter in scenes_in_chapters:\n",
    "    chapter_id = scenes_in_chapter['chapter_id']\n",
    "    scene_min, scene_max = scenes_in_chapter['scene_ids']\n",
    "    print(f\"Chapter #{chapter_id}: {scene_max - scene_min + 1} scenes\")\n",
    "\n",
    "    # update json files\n",
    "    for scene_id in range(scene_min, scene_max + 1):\n",
    "        shots_in_scenes[scene_id]['chapter_id'] = chapter_id\n",
    "        shot_min, shot_max = shots_in_scenes[scene_id]['shot_ids']\n",
    "        for shot_id in range(shot_min, shot_max + 1):\n",
    "            frames_in_shots[shot_id]['chapter_id'] = chapter_id\n",
    "            for frame_id in frames_in_shots[shot_id]['frame_ids']:\n",
    "                frame_embeddings[frame_id]['chapter_id'] = chapter_id\n",
    "\n",
    "# update the json files\n",
    "util.save_json_to_file('scenes_in_chapters.json', scenes_in_chapters)\n",
    "util.save_json_to_file('shots_in_scenes.json', shots_in_scenes)\n",
    "util.save_json_to_file('frames_in_shots.json', frames_in_shots)\n",
    "util.save_json_to_file('frame_embeddings.json', frame_embeddings)\n",
    "\n",
    "# plot the chapter images\n",
    "plot_chapters(frame_embeddings, len(scenes_in_chapters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd877b8-e455-4e02-8053-d1af1b4bee5e",
   "metadata": {},
   "source": [
    "### Generating chapter level contextual information #2: Contextual understandings with Anthropic Claude 3\n",
    "\n",
    "- download the IAB Content Taxonomy definitions\n",
    "- constructing prompt that includes composites images that describes each scene along with the conversation to Anthropic Claude 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844f9b5-5d5b-46f4-b7e6-4f38f0610375",
   "metadata": {},
   "source": [
    "#### Download the IAB Content Taxonomy definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bc500-bfad-4bd1-b0ee-3ffc34bc86d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iab_file = 'iab_content_taxonomy_v3.json'\n",
    "url = f\"https://dx2y1cac29mt3.cloudfront.net/iab/{iab_file}\"\n",
    "\n",
    "!curl {url} -o {iab_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41cf2ad-4456-4d98-ac1d-c8b12dc12bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_iab_taxonomies(file):\n",
    "    with open(file) as f:\n",
    "        iab_taxonomies = json.load(f)\n",
    "    return iab_taxonomies\n",
    "\n",
    "def get_chapter_frames(frame_embeddings, scenes_in_chapters):\n",
    "    num_chapters = len(scenes_in_chapters)\n",
    "    chapters_frames = [{\n",
    "        'chapter_id': i,\n",
    "        'text': '',\n",
    "        'frames': [],\n",
    "    } for i in range(num_chapters)]\n",
    "\n",
    "    for frame in frame_embeddings:\n",
    "        chapter_id = frame['chapter_id']\n",
    "        file = frame['file']\n",
    "        chapters_frames[chapter_id]['frames'].append(file)\n",
    "        chapters_frames[chapter_id]['text'] = scenes_in_chapters[chapter_id]['text']\n",
    "        \n",
    "    return chapters_frames\n",
    "\n",
    "\n",
    "\n",
    "def make_image_message(images):\n",
    "    # adding the composite image sequences\n",
    "    image_contents = [{\n",
    "        'type': 'text',\n",
    "        'text': 'Here are {0} images containing frame sequence that describes a scene.'.format(len(images))\n",
    "    }]\n",
    "\n",
    "    for image in images:\n",
    "        bas64_image = frames.image_to_base64(image)\n",
    "        image_contents.append({\n",
    "            'type': 'image',\n",
    "            'source': {\n",
    "                'type': 'base64',\n",
    "                'media_type': 'image/jpeg',\n",
    "                'data': bas64_image\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': image_contents\n",
    "    }\n",
    "\n",
    "def make_conversation_message(text):\n",
    "    message = {\n",
    "        'role': 'user',\n",
    "        'content': 'No conversation.'\n",
    "    }\n",
    "\n",
    "    if len(text) > 0:\n",
    "        message['content'] = 'Here is the conversation of the scene in <conversation> tag.\\n<conversation>\\n{0}\\n</conversation>\\n'.format(text)\n",
    "\n",
    "    return message\n",
    "\n",
    "def make_iab_taxonomoies(iab_list):\n",
    "    iab = [item['name'] for item in iab_list]\n",
    "    iab.append('None')\n",
    "\n",
    "    return {\n",
    "        'type': 'text',\n",
    "        'text': 'Here is a list of IAB Taxonomies in <iab> tag:\\n<iab>\\n${0}\\n</iab>\\nOnly answer the IAB taxonomy from this list.'.format('\\n'.join(iab))\n",
    "    }\n",
    "\n",
    "def make_garm_taxonomoies():\n",
    "    garm = [\n",
    "        'Adult & Explicit Sexual Content',\n",
    "        'Arms & Ammunition',\n",
    "        'Crime & Harmful acts to individuals and Society, Human Right Violations',\n",
    "        'Death, Injury or Military Conflict',\n",
    "        'Online piracy',\n",
    "        'Hate speech & acts of aggression',\n",
    "        'Obscenity and Profanity, including language, gestures, and explicitly gory, graphic or repulsive content intended to shock and disgust',\n",
    "        'Illegal Drugs, Tobacco, ecigarettes, Vaping, or Alcohol',\n",
    "        'Spam or Harmful Content',\n",
    "        'Terrorism',\n",
    "        'Debated Sensitive Social Issue',\n",
    "        'None',\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'type': 'text',\n",
    "        'text': 'Here is a list of GARM Taxonomies in <garm> tag:\\n<garm>\\n{0}\\n</garm>\\nOnly answer the GARM taxonomy from this list.'.format('\\n'.join(garm))\n",
    "    }\n",
    "\n",
    "def make_sentiments():\n",
    "    sentiments = ['Positive', 'Neutral', 'Negative', 'None']\n",
    "\n",
    "    return {\n",
    "        'type': 'text',\n",
    "        'text': 'Here is a list of Sentiments in <sentiment> tag:\\n<sentiment>\\n{0}\\n</sentiment>\\nOnly answer the sentiment from this list.'.format('\\n'.join(sentiments))\n",
    "    }\n",
    "\n",
    "def make_output_example():\n",
    "    example = {\n",
    "        'description': {\n",
    "            'text': 'The scene describes...',\n",
    "            'score': 98\n",
    "        },\n",
    "        'sentiment': {\n",
    "            'text': 'Positive',\n",
    "            'score': 90\n",
    "        },\n",
    "        'iab_taxonomy': {\n",
    "            'text': 'Station Wagon',\n",
    "            'score': 80\n",
    "        },\n",
    "        'garm_taxonomy': {\n",
    "            'text': 'Online piracy',\n",
    "            'score': 90\n",
    "        },\n",
    "        'brands_and_logos': [\n",
    "            {\n",
    "                'text': 'Amazon',\n",
    "                'score': 95\n",
    "            },\n",
    "            {\n",
    "                'text': 'Nike',\n",
    "                'score': 85\n",
    "            }\n",
    "        ],\n",
    "        'relevant_tags': [\n",
    "            {\n",
    "                'text': 'BMW',\n",
    "                'score': 95\n",
    "            }\n",
    "        ]            \n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': 'Return JSON format. An example of the output:\\n{0}\\n'.format(json.dumps(example))\n",
    "    }\n",
    "\n",
    "def get_contextual_information(images, text, iab_definitions):\n",
    "    model_id = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "    model_ver = 'bedrock-2023-05-31'\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "    task_all = 'You are asked to provide the following information: a detail description to describe the scene, identify the most relevant IAB taxonomy, GARM, sentiment, and brands and logos that may appear in the scene, and five most relevant tags from the scene.'\n",
    "    task_iab_only = 'You are asked to identify the most relevant IAB taxonomy.'\n",
    "    system = 'You are a media operation engineer. Your job is to review a portion of a video content presented in a sequence of consecutive images. Each image also contains a sequence of frames presented in a 4x7 grid reading from left to right and then from top to bottom. You may also optionally be given the conversation of the scene that helps you to understand the context. {0} It is important to return the results in JSON format and also includes a confidence score from 0 to 100. Skip any explanation.';\n",
    "\n",
    "    messages = []\n",
    " \n",
    "    # adding sequences of composite images to the prompt\n",
    "    message_images = make_image_message(images)\n",
    "    messages.append(message_images)\n",
    "\n",
    "    # adding the conversation to the prompt\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'Got the images. Do you have the conversation of the scene?'\n",
    "    })\n",
    "    message_conversation = make_conversation_message(text)\n",
    "    messages.append(message_conversation)\n",
    "\n",
    "    # other information\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'OK. Do you have other information to provdie?'\n",
    "    })\n",
    "\n",
    "    other_information = []\n",
    "    ## iab taxonomy\n",
    "    iab_list = make_iab_taxonomoies(iab_definitions['tier1'])\n",
    "    other_information.append(iab_list)\n",
    "\n",
    "    ## GARM\n",
    "    garm_list = make_garm_taxonomoies()\n",
    "    other_information.append(garm_list)\n",
    "\n",
    "    ## Sentiment\n",
    "    sentiment_list = make_sentiments()\n",
    "    other_information.append(sentiment_list)\n",
    "\n",
    "    messages.append({\n",
    "        'role': 'user',\n",
    "        'content': other_information\n",
    "    })\n",
    "\n",
    "    # output format\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'OK. What output format?'\n",
    "    })\n",
    "    output_format = make_output_example()\n",
    "    messages.append(output_format)\n",
    "\n",
    "    # prefill '{'\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': '{'\n",
    "    })\n",
    "    \n",
    "    model_params = {\n",
    "        'anthropic_version': model_ver,\n",
    "        'max_tokens': 4096,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.7,\n",
    "        'top_k': 20,\n",
    "        'stop_sequences': ['\\n\\nHuman:'],\n",
    "        'system': system.format(task_all),\n",
    "        'messages': messages\n",
    "    }\n",
    "\n",
    "\n",
    "    bedrock_runtime_client = boto3.client(service_name='bedrock-runtime')\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps(model_params),\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    # patch the json string output with '{' and parse it\n",
    "    response_content = response_body['content'][0]['text']\n",
    "    if response_content[0] != '{':\n",
    "        response_content = '{' + response_content\n",
    "    response_content = json.loads(response_content)\n",
    "    response_body['content'][0]['json'] = response_content\n",
    "\n",
    "    return response_body\n",
    "\n",
    "def display_contextual_cost(usage):\n",
    "    # us-east-1 pricing\n",
    "    input_per_1k = 0.00025\n",
    "    output_per_1k = 0.00125\n",
    "\n",
    "    input_tokens = usage['input_tokens']\n",
    "    output_tokens = usage['output_tokens']\n",
    "\n",
    "    contextual_cost = (\n",
    "        input_per_1k * input_tokens +\n",
    "        output_per_1k * output_tokens\n",
    "    ) / 1000\n",
    "\n",
    "    print('\\n')\n",
    "    print('========================================================================')\n",
    "    print('Estimated cost:', colored(f\"${round(contextual_cost, 4)}\", 'green'), f\"in us-east-1 region with {colored(input_tokens, 'green')} input tokens and {colored(output_tokens, 'green')} output tokens.\")\n",
    "    print('========================================================================')\n",
    "\n",
    "    return {\n",
    "        'input_per_1k': input_per_1k,\n",
    "        'output_per_1k': output_per_1k,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'estimated_cost': contextual_cost,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979340b2-de52-42de-9f0a-524933f7a323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_usage = {\n",
    "    'input_tokens': 0,\n",
    "    'output_tokens': 0,\n",
    "}\n",
    "\n",
    "iab_definitions = load_iab_taxonomies(iab_file)\n",
    "\n",
    "frames_in_chapters = get_chapter_frames(frame_embeddings, scenes_in_chapters)\n",
    "\n",
    "for frames_in_chapter in frames_in_chapters:\n",
    "    chapter_id = frames_in_chapter['chapter_id']\n",
    "    text = frames_in_chapter['text']\n",
    "    ch_frames = frames_in_chapter['frames']\n",
    "\n",
    "    composite_images = frames.create_composite_images(ch_frames)\n",
    "    num_images = len(composite_images)\n",
    "\n",
    "    for j in range(num_images):\n",
    "        composite_image = composite_images[j]\n",
    "        print(f\"Chapter #{chapter_id:02d}: {j + 1} of {num_images} composite images\")\n",
    "        w, h = composite_image.size\n",
    "        scaled = composite_image.resize((w // 4, h // 4))\n",
    "        display(scaled)\n",
    "\n",
    "    contextual_response = get_contextual_information(composite_images, text, iab_definitions)\n",
    "\n",
    "    usage = contextual_response['usage']\n",
    "    contextual = contextual_response['content'][0]['json']\n",
    "\n",
    "    # save the contextual to the chapter\n",
    "    scenes_in_chapters[chapter_id]['contextual'] = {\n",
    "        'usage': usage,\n",
    "        **contextual\n",
    "    }\n",
    "\n",
    "    total_usage['input_tokens'] += usage['input_tokens']\n",
    "    total_usage['output_tokens'] += usage['output_tokens']\n",
    "\n",
    "    print(f\"==== Chapter #{chapter_id:02d}: Contextual information ======\")\n",
    "    for key in ['description', 'sentiment', 'iab_taxonomy', 'garm_taxonomy']:\n",
    "        print(f\"{key.capitalize()}: {colored(contextual[key]['text'], 'green')} ({contextual[key]['score']}%)\")\n",
    "\n",
    "    for key in ['brands_and_logos', 'relevant_tags']:\n",
    "        items = ', '.join([item['text'] for item in contextual[key]])\n",
    "        if len(items) == 0:\n",
    "            items = 'None'\n",
    "        print(f\"{key.capitalize()}: {colored(items, 'green')}\")\n",
    "    print(f\"================================================\\n\\n\")\n",
    "\n",
    "util.save_json_to_file('scenes_in_chapters.json', scenes_in_chapters)\n",
    "\n",
    "contextual_cost = display_contextual_cost(total_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c80aad-a5db-496b-a15a-43cb9d934fee",
   "metadata": {},
   "source": [
    "### Total estimated cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee73b4-b408-425d-87d0-df938dd703c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_estimated_cost = 0\n",
    "\n",
    "for estimated_cost in [transcribe_cost, conversation_cost, frame_embeddings_cost, contextual_cost]:\n",
    "    total_estimated_cost += estimated_cost['estimated_cost']\n",
    "total_estimated_cost = round(total_estimated_cost, 4)\n",
    "\n",
    "print('\\n')\n",
    "print('\\n== Generating chapter points ===========================================\\n')\n",
    "print('Transcribe cost:', colored(f\"${round(transcribe_cost['estimated_cost'], 4)}\", 'green'), f\"with duration of {colored(transcribe_cost['duration'], 'green')}s\")\n",
    "print('Claude cost:', colored(f\"${round(conversation_cost['estimated_cost'], 4)}\", 'green'), f\"with {colored(conversation_cost['input_tokens'], 'green')} input tokens and {colored(conversation_cost['output_tokens'], 'green')} output tokens.\")\n",
    "print('\\n== Generating image embeddings =========================================\\n')\n",
    "print('Titan cost:', colored(f\"${round(frame_embeddings_cost['estimated_cost'], 4)}\", 'green'), f\"with {colored(frame_embeddings_cost['num_embeddings'], 'green')} embeddings.\")\n",
    "print('\\n== Chapter contextual information ======================================\\n')\n",
    "print('Claude cost:', colored(f\"${round(contextual_cost['estimated_cost'], 4)}\", 'green'), f\"with {colored(contextual_cost['input_tokens'], 'green')} input tokens and {colored(contextual_cost['output_tokens'], 'green')} output tokens.\")\n",
    "print('\\n========================================================================\\n')\n",
    "print('Total estimated cost:', colored(f\"${total_estimated_cost}\", 'green'))\n",
    "print('\\n========================================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f4ca9b-9ac1-4c9a-80f0-9d63e72359bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
