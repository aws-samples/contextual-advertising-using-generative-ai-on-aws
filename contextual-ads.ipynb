{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6416fcdf-75c5-49fc-a9ec-adb8018df284",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scene and Ad break detection and contextual understanding for advertising using Generative AI on AWS\n",
    "\n",
    "Contextual advertising is a form of targeted advertising where the advertisement is matched to the context of the webpage or media being consumed by the user. This process involves three key players: the publisher (website or content owner), the advertiser, and the consumer. Publishers provide the platform and content, while advertisers create ads tailored to the context. Consumers engage with the content, and relevant ads are displayed based on the context, creating a more personalized and relevant advertising experience.\n",
    "\n",
    "One particularly challenging area of contextual advertising is inserting ads in media content for streaming on video on demand (VOD) platforms. This process traditionally relied on manual tagging, where human experts analyze the content and assign relevant keywords or categories. However, this approach is time-consuming, subjective, and may not capture the full context or nuances of the content. Traditional AI/ML solutions can automate this process, but they often require extensive training data and can be expensive and limited in their capabilities.\n",
    "\n",
    "Generative AI, powered by large language models, offers a promising solution to this challenge. By leveraging the vast knowledge and contextual understanding of these models, broadcasters and content producers can automatically generate contextual insights and taxonomies for their media assets. This approach not only streamlines the process but also provides more accurate and comprehensive contextual understanding, enabling more effective ad targeting and monetization of media archives.\n",
    "\n",
    "In this project, we will do a deep dive into one of the new features of the [Guidance for Media2Cloud on AWS V4](https://github.com/aws-solutions-library-samples/guidance-for-media2cloud-on-aws), Scene and Ad break detection and contextual understandings of the Ad break. We will demonstrate step by step how to create contextual relevant insights and taxonomies for advertising using generative AI on AWS. This will allow broadcasters and content producers to monetize their media assets more effectively and extract greater value from their media archives. By harnessing the power of generative AI, they can unlock new revenue streams and deliver more personalized and engaging advertising experiences to their audiences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d95673-a105-4e1c-a746-237773cb33b7",
   "metadata": {},
   "source": [
    "Watch the demo video of Ad break detection in Media2Cloud, https://www.youtube.com/watch?v=s9PMP1Gi7Ag\n",
    "\n",
    "\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=s9PMP1Gi7Ag\" target=\"_blank\">\n",
    " <img src=\"http://img.youtube.com/vi/s9PMP1Gi7Ag/mqdefault.jpg\" alt=\"Watch the video\" width=\"240\" height=\"180\" border=\"10\" />\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d011bfa-9251-402b-b1a3-7b28aca93131",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Key Terms and Definitions\n",
    "\n",
    "- **Frame** - frame image extracted from the video content\n",
    "- **Shot** - continuous sequences of frames between two edits or cuts that defines one action\n",
    "- **Scene** - continuous sequence of action taking place in a specific location and time, consisting of a series of shots.\n",
    "- **Chapter** - logical divisions of the storyline of the video content, consisting of a series of shots and conversations on the similar topic\n",
    "- **WebVTT** - a file format used to store timed text track data, such as subtitles or captions, for video content on the web.\n",
    "- **The Interactive Advertising Bureau (IAB) Content Taxonomy** - standardized metadata categories and subcategories that enable advertising platforms, publishers, and advertisers to effectively target and match ads with relevant content\n",
    "- **Global Alliance for Responsible Media (GARM) Taxonomy** - standardized categorization that defines sensitive content topics that advertisers can avoid or apply specific brand suitability settings for in digital advertising.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2fe83-faea-4cfe-b1ba-89fe5ce86335",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Solution Overview\n",
    "\n",
    "In our pursuit to achieve optimum design, we tested multiple techniques, including self-hosted image caption models, \n",
    "leveraging large language models to summarize transcriptions and detected labels, utilizing in-context learning to classify \n",
    "scene summaries according to the IAB Content Taxonomy Version 3, and harnessing embeddings for embedding search. \n",
    "During this intensive testing period, we witnessed a remarkable pace of advancement in generative AI. \n",
    "The models rapidly evolved, becoming faster, more cost-effective, and increasingly capable. This allowed us to finally \n",
    "converge on a design below that harnesses the cutting-edge Anthropic Claude 3 Multi-modal foundation model.\n",
    "\n",
    "\n",
    "#### Workflow steps\n",
    "\n",
    "1. Setup prequisite and upload a media asset to Amazon Simple Storage Service (S3).\n",
    "2. Generate the audio chapter points: we use Amazon Transcribe, Automatic Speech Recognition (ASR) service to generate transcription from the audio dialogues of the media asset. then use Anthropic's Claude 3 Haiku model to analyze the conversation and identify chapter points based on significantly topic changes.\n",
    "3. In parallel, generate scene grid from video frames: we sample the frames from video and use Amazon Titan Multimodal Embedding model to help group frames into shots and then group shots into scenes based on visual similarity.\n",
    "4. Align scene and audio chapter: align video scenes with the audio chapters to identify un-intrusive breaks for ad insertion\n",
    "5. Generate the contextual response: We send the the scene grid, transcription, to Anthropic Claude 3 model in Amazon Bedrock to generate relevant contextual response: such as scene description, sentiment, relevant IAB or any other custom taxonomy.\n",
    "\n",
    "In this sample notebook we will walk through these steps.  Lets dive in!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c0d08-07c0-4da7-adea-aa011894b5a1",
   "metadata": {},
   "source": [
    "## 1. Setup Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488c3b7-2901-497e-8a1e-10d768f31943",
   "metadata": {},
   "source": [
    "### Install and import python packages\n",
    "\n",
    "- ffmpeg for video and image processing\n",
    "- faiss for vector store\n",
    "- webvtt-py for parsing subtitle file\n",
    "- termcolor for formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a72175-ec2b-4442-8bd6-b9e12ca5019f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453ab94-9b8a-42fe-a0e5-2b2e742d71fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from termcolor import colored\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from termcolor import colored\n",
    "import glob\n",
    "import os\n",
    "from functools import cmp_to_key\n",
    "from lib import transcribe_helper as trh\n",
    "from lib import s3_helper as s3h\n",
    "from lib import chapters as chpt\n",
    "from lib import util\n",
    "from lib import embeddings\n",
    "from lib import frames\n",
    "from lib import ffmpeg_helper as ffh\n",
    "from lib import bedrock_helper as brh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db107e-04e8-4604-9286-dac677a2eed8",
   "metadata": {},
   "source": [
    "### Download the sample video, Meridian, from Netflix\n",
    "\n",
    "The open source content is available under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aca4dd-42c6-4da0-94c5-263e7bd42ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "mp4_file = 'Netflix_Open_Content_Meridian.mp4'\n",
    "video_dir = Path(mp4_file).stem\n",
    "\n",
    "url = f\"https://dx2y1cac29mt3.cloudfront.net/mp4/netflix/{mp4_file}\"\n",
    "\n",
    "!curl {url} -o {mp4_file}\n",
    "\n",
    "Video(mp4_file, width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aac370-2f9f-4989-b437-b9ca58825b22",
   "metadata": {},
   "source": [
    "### Get Sagemaker default resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7344ae2-2380-4789-94ee-0094007e7f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sagemaker_resources = {}\n",
    "sagemaker_resources[\"session\"] = sagemaker.Session()\n",
    "sagemaker_resources[\"bucket\"] = sagemaker_resources[\"session\"].default_bucket()\n",
    "sagemaker_resources[\"role\"] = sagemaker.get_execution_role()\n",
    "sagemaker_resources[\"region\"] = sagemaker_resources[\"session\"]._region_name\n",
    "\n",
    "print(sagemaker_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644e0bb-3d2f-4668-b69e-a1ad18c2eecd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload the sample video to the default Amazon S3 bucket for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd992c-c1be-4f98-a60c-dc0d87df5f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "response = s3h.upload_object(sagemaker_resources[\"bucket\"], \"contextual_ad\", mp4_file) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf4d06-855a-41e3-8fc6-716db94c024b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Generate chapter segments based on the dialog in the video\n",
    "\n",
    "Once the video is uploaded to S3, we will leverage Amazon Transcribe and a foundation model from Bedrock to automatically generate conversational chapter points. This will help us keep track of when conversation topics start and end in the video. The process begins with Amazon Transcribe converting speech to text and generating a transcription. This transcription is then downloaded and formatted into the WebVTT format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05b011-a6e9-4d89-a122-1bd4d881c568",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use Amazon Transcribe to convert speech to text\n",
    "\n",
    "This section uses Amazon Transcribe to convert the speech to text and generate a WebVTT output.\n",
    "\n",
    "If you are getting `AccessDeniedException`, log on to `AWS IAM Console`, find the SageMaker Execution IAM Role, and add the following managed polices:\n",
    "- AmazonTranscribeFullAccess\n",
    "- AmazonRekognitionFullAccess\n",
    "- AmazonBedrockFullAccess\n",
    "\n",
    "Also check out the pricing on [Amazon Transcribe Pricing](https://aws.amazon.com/transcribe/pricing/) in us-east-1 region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12d43a-ea2c-4e20-bda1-8265ae19b4c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'SageMaker execution IAM Role ARN: {sagemaker_resources[\"role\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163f75d-93ce-4404-9c66-5edf94e2537b",
   "metadata": {},
   "source": [
    "### Probe the video to get the stream information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc04cd-e27a-4592-8af0-b4cc2156a554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stream_info = ffh.probe_stream(mp4_file)\n",
    "\n",
    "JSON(stream_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e15b6-ee3a-4884-ae87-0dd6aafdaa24",
   "metadata": {},
   "source": [
    "### Start the transcription job and wait for it to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad04daa-1a25-46ca-8822-fe9792fd111b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start transcription job\n",
    "\n",
    "transcribe_response = trh.transcribe(sagemaker_resources[\"bucket\"], \"contextual_ad\", mp4_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb594a-a188-425e-9f45-722fd0243b3a",
   "metadata": {},
   "source": [
    "### Examine the results from Amazon Transcribe\n",
    "\n",
    "The response from Amazon Transcribe contains a `results` dictionary with a `transcript` that contains a text-only transcript and a collection of `items` which contain each word and punctuation in the transcript along with a confidence score and timestamp for the item. The response also contains the same transcript formatted as subtitles in either WebVTT or SRT format.  Let's take a look at these outputs.  \n",
    "\n",
    "We will be using the WebVTT output for our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f39a8-2b3b-4782-a617-dad8f3d0752e",
   "metadata": {},
   "source": [
    "**Transcript**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75fdf1-996f-499a-9f3a-6aee39c71062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_filename = trh.download_transcript(transcribe_response, output_dir = video_dir)\n",
    "\n",
    "JSON(filename=transcript_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1582f-2dde-44d6-aed4-b9ce880e0557",
   "metadata": {
    "tags": []
   },
   "source": [
    "**WebVTT Subtitles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03f27e-2830-4db8-86dc-44a267878f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vtt_filename = trh.download_vtt(transcribe_response, output_dir = video_dir)\n",
    "\n",
    "!head {vtt_filename}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6277d-19bf-41b2-804e-0144f3597914",
   "metadata": {},
   "source": [
    "### Estimate the cost of the transcription job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107506fb-2992-4102-aa1e-2bbd8b57ef8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "duration_ms = stream_info['video_stream']['duration_ms']\n",
    "transcribe_cost = trh.display_transcription_cost(duration_ms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaef380-0974-48c3-866a-82c47f366d28",
   "metadata": {},
   "source": [
    "### Use an Amazon Bedrock to generate chapters from the subtitles \n",
    "Next, the transcript is passed to the Anthropic Claude 3 Haiku model from Amazon Bedrock. The model analyzes the transcript and suggests conversational chapter points in a specific JSON format. In the prompt, we specify that each chapter should contain a start and end timestamp along with a reason describing the topic. The prompts for the Haiku model are shown below:\n",
    "\n",
    "**System prompt**\n",
    "\n",
    "```\n",
    "You are a media operation assistant who analyses movie transcripts in WebVTT \n",
    "format and suggest chapter points based on the topic changes in the conversations. \n",
    "It is important to read the entire transcripts.\n",
    "```\n",
    "\n",
    "\n",
    "**Messages**\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        'content': 'Here is the transcripts in <transcript> tag:\\n'\n",
    "                '<transcript>{transcript}\\n</transcript>\\n',\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'content': 'OK. I got the transcript. What output format?',\n",
    "        'role': 'assistant'\n",
    "    },\n",
    "    {\n",
    "        'content': 'JSON format. An example of the output:\\n'\n",
    "                '{\"chapters\": [{\"start\": \"00:00:10.000\", \"end\": \"00:00:32.000\", '\n",
    "                '\"reason\": \"It appears the chapter talks about...\"}]}\\n',\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'content': '{', 'role': 'assistant'\n",
    "    }\n",
    " ]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b1ff6-0b91-489c-8b9a-0438292f7b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "conversation_response = brh.analyze_conversations(vtt_filename)\n",
    "\n",
    "# show the conversation cost\n",
    "conversation_cost = brh.display_conversation_cost(conversation_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990df82c-eb67-4a84-8ce7-4499cb44f326",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's take a look at the conversations that were generated from the transcript \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906eced6-682a-4578-8a60-a179b93d529b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversations = conversation_response['content'][0]['json']\n",
    "\n",
    "JSON(conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c93f658-0d0f-4513-ba36-44bc843d1c0b",
   "metadata": {},
   "source": [
    "### Generate \"chapter points\" \n",
    "\n",
    "To ensure the model's output accurately reflects the original transcript, the output JSON is post-processed to merge any overlapping chapter timestamps and align the chapter boundaries with the actual caption timestamps from the WebVTT file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdebcd7-3e40-42a9-88ff-9781abba916d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## parse the conversation\n",
    "conversations = conversation_response['content'][0]['json']\n",
    "\n",
    "## merge overlapped conversation timestamps\n",
    "chapters = chpt.merge_chapters(conversations['chapters'])\n",
    "\n",
    "## validate the conversation timestamps against the caption timestamps\n",
    "captions = chpt.parse_webvtt(vtt_filename)\n",
    "chapters = chpt.validate_timestamps(chapters, captions)\n",
    "\n",
    "conversations['chapters'] = chapters\n",
    "\n",
    "## save the conversations\n",
    "util.save_to_file(os.path.join(video_dir, 'conversations.json'), conversations)\n",
    "\n",
    "JSON(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3536067-54b4-457c-9402-4eda7c34933f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimated_cost = conversation_cost['estimated_cost'] + transcribe_cost['estimated_cost']\n",
    "estimated_cost = round(estimated_cost, 4)\n",
    "\n",
    "print('\\n')\n",
    "print('Generating \"chapter points\"')\n",
    "print('========================================================================')\n",
    "print('Transcribe cost:', colored(f\"${round(transcribe_cost['estimated_cost'], 4)}\", 'green'), f\"with duration of {colored(transcribe_cost['duration'], 'green')}s\")\n",
    "print('Bedrock cost:', colored(f\"${round(conversation_cost['estimated_cost'], 4)}\", 'green'), f\"with {colored(conversation_cost['input_tokens'], 'green')} input tokens and {colored(conversation_cost['output_tokens'], 'green')} output tokens.\")\n",
    "print('-----')\n",
    "print('Estimated cost:', colored(f\"${estimated_cost}\", 'green'))\n",
    "print('========================================================================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f6196f-74df-4941-950e-e7f030396ec7",
   "metadata": {},
   "source": [
    "## CHECKPOINT\n",
    "\n",
    "At this point, we have taken the audio part of the video file, run Amazon Transcribe to convert the speech to text, and run Amazon Bedrock (Anthropic Claude 3 Haiku) model to analyze the conversations.\n",
    "\n",
    "Let's move on to analyzing the visual part of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7871c9-11aa-4ddc-bd9b-2a2ec0931e7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Generate a scene grid from video frames\n",
    "\n",
    "In this section, we will sample the frames from the video and use Amazon Titan Multimodal Embedding (TME) model to help group frames into shots and then group shots into scenes based on visual similarity. Each frame from a scene is used to create a single composite image or \"scene grid\" that can be used as an input to Amazon Bedrock to understand the scene. \n",
    "\n",
    "In this process, we first sample one frame per second, then employ a cosine similarity logic on the adjacent frames to group frame images into shots, which represent camera shot change events. We chose one frame per second for downsampling based on past experiences, but this can be calibrated if you have high-motion, high-frame-rate videos. \n",
    "\n",
    "Even after identifying individual camera shots, there may still be too many semantically similar shots depicting the same setting. To further cluster these into distinct scenes, we need to expand our frame comparison beyond just adjacent frames. By looking at similar frames across an expanded time window, we can identify shots that are likely part of the same contiguous scene. We calculate pairwise similarity scores between all frames within a given time window. Frames with similarity scores above a certain threshold are considered part of the same scene group. This process is performed recursively across all frames in a shot. The time window size and similarity threshold are calibrated parameters that can significantly impact scene boundary detection accuracy. In our example, we found a 3-minute time window and 0.85 similarity threshold gave the best scene clustering results across our video samples.\n",
    "\n",
    "Technically, this scene grouping process is accomplished by first indexing all video frames using TME again and storing the embeddings along with their shot information and timestamps into a vector database, as illustrated in the figure below.  For this notebok, we are using a FAIS vector store to manage embedding locally, but you can use any vector store.  The implementation in Solution Guidance for Media2Cloud on AWS uses Amazon Open Search Serverless for this purpose.\n",
    "\n",
    "![scene grouping](./static/images/scene-grouping.png)\n",
    "\n",
    "**NOTE:** In an automated workflow, this step can be run in parallel to generating chapter points since there is no dependency between the steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394d108-a4fe-4b5a-8530-7fa6ac79f970",
   "metadata": {},
   "source": [
    "### Sample frames from the video\n",
    "\n",
    "In this section, we are extracting 1 frame per second with a resolution of `392x220` from the sample video. Using `392x220` is chosen for a reason and will be discussed in \"Generating chapter level contextual information\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c3d27-06d7-420d-a93c-bb05cea95223",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "jpeg_files = ffh.extract_frames(mp4_file, stream_info, (392, 220))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f9b3c-b196-43cc-8ead-c7cea8b0819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Frame extracted: {len(jpeg_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b942415-e5b5-48d5-bac2-5d5ce37c3e91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Group frames into shots with Amazon Titan Multimodal Embedding\n",
    "\n",
    "- Generate frame embeddings with Amazon Titan Multimodal Embedding model\n",
    "- Group frames into shots with cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be9bc8-5a9d-4848-9143-aa5f8eb0396f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate frame embeddings with Amazon Titan Multimodal Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b8675d-83f9-4597-b8b0-62275e32d269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_embeddings = embeddings.batch_generate_embeddings(jpeg_files, output_dir = video_dir)\n",
    "\n",
    "frame_embeddings_cost = embeddings.display_embedding_cost(frame_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c729fc7-a7a8-472b-a4b2-c586dddf29a5",
   "metadata": {},
   "source": [
    "#### Group adjacent frames into shots with cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe134c1-a469-4d52-be67-839c45d1e45f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames_in_shots = frames.group_frames_to_shots(frame_embeddings)\n",
    "\n",
    "print(f\"Number of shots: {len(frames_in_shots)} from {len(frame_embeddings)} frames\")\n",
    "\n",
    "# update shot_id in frame_embeddings dict\n",
    "for idx, frames_in_shot in enumerate(frames_in_shots):\n",
    "    for frame_id in frames_in_shot['frame_ids']:\n",
    "        frame_embeddings[frame_id]['shot_id'] = idx\n",
    "\n",
    "# save to json file\n",
    "for file, data in [\n",
    "    ('frames_in_shots.json', frames_in_shots),\n",
    "    ('frame_embeddings.json', frame_embeddings)\n",
    "]:\n",
    "    output_file = os.path.join(video_dir, file)\n",
    "    util.save_to_file(output_file, data)\n",
    "\n",
    "# plot the shot images\n",
    "frames.plot_shots(frame_embeddings, len(frames_in_shots))\n",
    "\n",
    "print('========')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33680b6a-3944-4a0c-9bf4-667807afa609",
   "metadata": {},
   "source": [
    "### Group shots into scenes using similarity search\n",
    "\n",
    "The previous step (grouping frames to shots) compares the similarity of the adjacent frames. This step compares the frames to the rest of the frame images of the entire content. This allows us to group frame images that are further apart to group the shots into scenes.\n",
    "\n",
    "We will perform a recursive similarity search against this indexed frame corpus. For each frame, we find all other frames within a 3-minute time window that have greater than 85% contextual similarity based on their vector representations. The shot information for these highly similar frames is recorded. This process iterates across all frames within each shot. Finally, we group the shot information that were mutually identified as highly similar into distinct scene groups. This allows us to segment the initially detected shot boundaries into higher-level semantic scene boundaries based on visual and temporal coherence.\n",
    "\n",
    "![shots to scenes](./static/images/shots-to-scenes.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac1ade-86e6-4f4f-b3b6-d45d1a89ec01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## create an index\n",
    "dimension = len(frame_embeddings[0]['embedding'])\n",
    "vector_store = embeddings.create_index(dimension)\n",
    "\n",
    "## indexing all the frames\n",
    "embeddings.index_frames(vector_store, frame_embeddings)\n",
    "print(f\"Total indexed = {vector_store.ntotal}\")\n",
    "\n",
    "## find similar frames for each of the frames and store in the frame_embeddings\n",
    "for frame in frame_embeddings:\n",
    "    similar_frames = embeddings.search_similarity(vector_store, frame)\n",
    "    frame['similar_frames'] = similar_frames\n",
    "\n",
    "## find all similar frames that are related to the shots and store in the frames_in_shots\n",
    "for frames_in_shot in frames_in_shots:\n",
    "    similar_frames_in_shot = frames.collect_similar_frames(frame_embeddings, frames_in_shot['frame_ids'])\n",
    "    frames_in_shot['similar_frames_in_shot'] = similar_frames_in_shot\n",
    "\n",
    "    related_shots = frames.collect_related_shots(frame_embeddings, similar_frames_in_shot)\n",
    "    frames_in_shot['related_shots'] = related_shots\n",
    "\n",
    "shots_in_scenes = frames.group_shots_in_scenes(frames_in_shots)\n",
    "\n",
    "# store the scene_id to all structs\n",
    "for scene in shots_in_scenes:\n",
    "    scene_id = scene['scene_id']\n",
    "    shot_min, shot_max = scene['shot_ids']\n",
    "    print(f\"Scene #{scene_id}: {shot_min} - {shot_max} ({shot_max - shot_min + 1})\")\n",
    "    # update json files\n",
    "    for shot_id in range(shot_min, shot_max + 1):\n",
    "        frames_in_shots[shot_id]['scene_id'] = scene_id\n",
    "        for frame_id in frames_in_shots[shot_id]['frame_ids']:\n",
    "            frame_embeddings[frame_id]['scene_id'] = scene_id\n",
    "\n",
    "# update the json files\n",
    "# save to json file\n",
    "for file, data in [\n",
    "    ('shots_in_scenes.json', shots_in_scenes),\n",
    "    ('frames_in_shots.json', frames_in_shots),\n",
    "    ('frame_embeddings.json', frame_embeddings)\n",
    "]:\n",
    "    output_file = os.path.join(video_dir, file)\n",
    "    util.save_to_file(output_file, data)\n",
    "\n",
    "# plot the scene images\n",
    "frames.plot_scenes(frame_embeddings, len(shots_in_scenes))\n",
    "\n",
    "print(f\"Number of frames: {len(frame_embeddings)}\")\n",
    "print(f\"Number of shots: {len(frames_in_shots)}\")\n",
    "print(f\"Number of scenes: {len(shots_in_scenes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca044876-3b08-4113-8ab2-448d70c0ca06",
   "metadata": {},
   "source": [
    "## 4. Align scene and chapter\n",
    "\n",
    "At this point, we have separately processed the visual and audio cues from the video. Now, we bring them together and ensure that the transcription chapters align with the scene breaks. The last thing you want is to insert an ad during an ongoing conversation or scene. To create alignment, we will iterate over each conversational chapter, represented by its start and end timestamps, and a text description summarizing the topic. For each chapter, the code identifies the relevant video scenes that overlap or fall within the chapter's timestamp range. The output of this process is a list of chapters, where each chapter contains a list of scene IDs representing the video scenes that align with the corresponding audio conversation. After the alignment process, we have combined visual and audio cues into the final chapters. The breaks we identified are what the system suggested as ideal places for ad insertion. In real-world applications, we recommend surfacing these breaks as suggestions to the operator and having a human-in-the-loop step to confirm the final breaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d0967-a52f-4cc4-bfdf-9c2f27db78e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scenes_in_chapters = frames.group_scenes_in_chapters(\n",
    "    conversations,\n",
    "    shots_in_scenes,\n",
    "    frames_in_shots\n",
    ")\n",
    "\n",
    "for scenes_in_chapter in scenes_in_chapters:\n",
    "    chapter_id = scenes_in_chapter['chapter_id']\n",
    "    scene_min, scene_max = scenes_in_chapter['scene_ids']\n",
    "    print(f\"Chapter #{chapter_id}: {scene_max - scene_min + 1} scenes\")\n",
    "\n",
    "    # update json files\n",
    "    for scene_id in range(scene_min, scene_max + 1):\n",
    "        shots_in_scenes[scene_id]['chapter_id'] = chapter_id\n",
    "        shot_min, shot_max = shots_in_scenes[scene_id]['shot_ids']\n",
    "        for shot_id in range(shot_min, shot_max + 1):\n",
    "            frames_in_shots[shot_id]['chapter_id'] = chapter_id\n",
    "            for frame_id in frames_in_shots[shot_id]['frame_ids']:\n",
    "                frame_embeddings[frame_id]['chapter_id'] = chapter_id\n",
    "\n",
    "# update the json files\n",
    "for file, data in [\n",
    "    ('scenes_in_chapters.json', scenes_in_chapters),\n",
    "    ('shots_in_scenes.json', shots_in_scenes),\n",
    "    ('frames_in_shots.json', frames_in_shots),\n",
    "    ('frame_embeddings.json', frame_embeddings),\n",
    "]:\n",
    "    output_file = os.path.join(video_dir, file)\n",
    "    util.save_to_file(output_file, data)\n",
    "\n",
    "# plot the chapter images\n",
    "frames.plot_chapters(frame_embeddings, len(scenes_in_chapters))\n",
    "\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd877b8-e455-4e02-8053-d1af1b4bee5e",
   "metadata": {},
   "source": [
    "## 5. Generate chapter level contextual information \n",
    "\n",
    "The last step is to send both the visually and audio-aligned data to Claude 3 Haiku to generate contextual information for each chapter. This is an innovative approach that takes advantage of the multimodal capabilities of the Claude 3 family of models. From our testing, these models have demonstrated the ability to capture minute details from large images and follow image sequences when provided with appropriate instructions.\n",
    "\n",
    "To prepare the input for Claude3 Haiku, we first assemble video frames associated with each chapter and create a composite image grid. Through our experimentation, we have found that the optimum image grid ratio is 7 rows by 4 columns, which will assemble a 1568 x 1540 pixel image that fits under Claude's 5 MB image file size limit while still preserving enough detail in each individual frame tile. Furthermore, you can also assemble multiple images if needed.\n",
    "\n",
    "Subsequently, the composite images, the transcription, the IAB Content taxonomy definitions, and GARM taxonomy definitions are fed into the prompt to generate descriptions, sentiment, IAB taxonomy, GARM taxonomy, and other relevant information in a single query to the Claude3 Haiku model. Not only that, but we can adapt this approach to any taxonomy or custom labeling use cases without the need to train a model each time. This is where the true power of this approach lies. The final output can be presented to a human reviewer for final confirmation if needed. Here is an example of a composite image grid and the corresponding contextual output for a specific chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844f9b5-5d5b-46f4-b7e6-4f38f0610375",
   "metadata": {},
   "source": [
    "#### Download the IAB Content Taxonomy definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bc500-bfad-4bd1-b0ee-3ffc34bc86d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iab_file = 'iab_content_taxonomy_v3.json'\n",
    "url = f\"https://dx2y1cac29mt3.cloudfront.net/iab/{iab_file}\"\n",
    "\n",
    "!curl {url} -o {iab_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c7ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_iab_taxonomies(file):\n",
    "    with open(file) as f:\n",
    "        iab_taxonomies = json.load(f)\n",
    "    return iab_taxonomies\n",
    "\n",
    "def get_chapter_frames(frame_embeddings, scenes_in_chapters):\n",
    "    num_chapters = len(scenes_in_chapters)\n",
    "    chapters_frames = [{\n",
    "        'chapter_id': i,\n",
    "        'text': '',\n",
    "        'frames': [],\n",
    "    } for i in range(num_chapters)]\n",
    "\n",
    "    for frame in frame_embeddings:\n",
    "        chapter_id = frame['chapter_id']\n",
    "        file = frame['file']\n",
    "        chapters_frames[chapter_id]['frames'].append(file)\n",
    "        chapters_frames[chapter_id]['text'] = scenes_in_chapters[chapter_id]['text']\n",
    "        \n",
    "    return chapters_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73db58-48b9-4814-998d-42a648dc0b55",
   "metadata": {},
   "source": [
    "### Create composte images and use Anthropic Claude to generate contextual information for each chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979340b2-de52-42de-9f0a-524933f7a323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_usage = {\n",
    "    'input_tokens': 0,\n",
    "    'output_tokens': 0,\n",
    "}\n",
    "\n",
    "iab_definitions = load_iab_taxonomies(iab_file)\n",
    "\n",
    "frames_in_chapters = get_chapter_frames(frame_embeddings, scenes_in_chapters)\n",
    "\n",
    "for frames_in_chapter in frames_in_chapters:\n",
    "    chapter_id = frames_in_chapter['chapter_id']\n",
    "    text = frames_in_chapter['text']\n",
    "    ch_frames = frames_in_chapter['frames']\n",
    "\n",
    "    composite_images = frames.create_composite_images(ch_frames)\n",
    "    num_images = len(composite_images)\n",
    "\n",
    "    for j in range(num_images):\n",
    "        composite_image = composite_images[j]\n",
    "        print(f\"Chapter #{chapter_id:02d}: {j + 1} of {num_images} composite images\")\n",
    "        w, h = composite_image.size\n",
    "        scaled = composite_image.resize((w // 4, h // 4))\n",
    "        display(scaled)\n",
    "\n",
    "    contextual_response = brh.get_contextual_information(composite_images, text, iab_definitions)\n",
    "    \n",
    "    # close the images\n",
    "    for composite_image in composite_images:\n",
    "        composite_image.close()\n",
    "\n",
    "    usage = contextual_response['usage']\n",
    "    contextual = contextual_response['content'][0]['json']\n",
    "\n",
    "    # save the contextual to the chapter\n",
    "    scenes_in_chapters[chapter_id]['contextual'] = {\n",
    "        'usage': usage,\n",
    "        **contextual\n",
    "    }\n",
    "\n",
    "    total_usage['input_tokens'] += usage['input_tokens']\n",
    "    total_usage['output_tokens'] += usage['output_tokens']\n",
    "\n",
    "    print(f\"==== Chapter #{chapter_id:02d}: Contextual information ======\")\n",
    "    for key in ['description', 'sentiment', 'iab_taxonomy', 'garm_taxonomy']:\n",
    "        print(f\"{key.capitalize()}: {colored(contextual[key]['text'], 'green')} ({contextual[key]['score']}%)\")\n",
    "\n",
    "    for key in ['brands_and_logos', 'relevant_tags']:\n",
    "        items = ', '.join([item['text'] for item in contextual[key]])\n",
    "        if len(items) == 0:\n",
    "            items = 'None'\n",
    "        print(f\"{key.capitalize()}: {colored(items, 'green')}\")\n",
    "    print(f\"================================================\\n\\n\")\n",
    "\n",
    "output_file = os.path.join(video_dir, 'scenes_in_chapters.json')\n",
    "util.save_to_file(output_file, scenes_in_chapters)\n",
    "\n",
    "contextual_cost = brh.display_contextual_cost(total_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c80aad-a5db-496b-a15a-43cb9d934fee",
   "metadata": {},
   "source": [
    "### Total estimated cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee73b4-b408-425d-87d0-df938dd703c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_estimated_cost = 0\n",
    "\n",
    "for estimated_cost in [transcribe_cost, conversation_cost, frame_embeddings_cost, contextual_cost]:\n",
    "    total_estimated_cost += estimated_cost['estimated_cost']\n",
    "total_estimated_cost = round(total_estimated_cost, 4)\n",
    "\n",
    "print('\\n')\n",
    "print('\\n== Generating chapter points ===========================================\\n')\n",
    "print('Transcribe cost:', colored(f\"${round(transcribe_cost['estimated_cost'], 4)}\", 'green'), f\"with duration of {colored(transcribe_cost['duration'], 'green')}s\")\n",
    "print('Claude cost:', colored(f\"${round(conversation_cost['estimated_cost'], 4)}\", 'green'), f\"with {colored(conversation_cost['input_tokens'], 'green')} input tokens and {colored(conversation_cost['output_tokens'], 'green')} output tokens.\")\n",
    "print('\\n== Generating image embeddings =========================================\\n')\n",
    "print('Titan cost:', colored(f\"${round(frame_embeddings_cost['estimated_cost'], 4)}\", 'green'), f\"with {colored(frame_embeddings_cost['num_embeddings'], 'green')} embeddings.\")\n",
    "print('\\n== Chapter contextual information ======================================\\n')\n",
    "print('Claude cost:', colored(f\"${round(contextual_cost['estimated_cost'], 4)}\", 'green'), f\"with {colored(contextual_cost['input_tokens'], 'green')} input tokens and {colored(contextual_cost['output_tokens'], 'green')} output tokens.\")\n",
    "print('\\n========================================================================\\n')\n",
    "print('Total estimated cost:', colored(f\"${total_estimated_cost}\", 'green'))\n",
    "print('\\n========================================================================')\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
